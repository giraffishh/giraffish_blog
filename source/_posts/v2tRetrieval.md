---
index_img: 'https://origin.picgo.net/2025/09/21/25-09-21-17584556830165f89cdc8dfd4716f.webp'
banner_img: 'https://img.picgo.net/2025/03/28/25-03-28-1743150706296586d07573c738663.webp'
title: Text-to-Video Retrieval
categories:
  - 学习笔记
tags:
  - 视频文本检索
  - 模型蒸馏
comments: true
abbrlink: 9d7503d7
date: 2025-09-22 00:55:53
updated: 2025-09-23 00:23:27
---

## CLIP

Learning Transferable Visual Models From Natural Language Supervision

* 论文链接：https://arxiv.org/pdf/2103.00020
* 代码仓库：https://github.com/OpenAI/CLIP

![](https://origin.picgo.net/2025/09/22/25-09-22-17584720224735adc63e5fb736e0c.webp)

### 核心架构组件

**图像编码器 (Image Encoder)：**

* **作用**：将输入的图像转换成一个数学向量（特征嵌入），这个向量代表了图像的内容
* **具体模型**：论文中测试了两种主流的架构 ：
  1. **ResNet-50**：一个经过改进的经典卷积神经网络（CNN）
  2. **Vision Transformer (ViT)**：一种更新的、基于 Transformer 的架构

**文本编码器 (Text Encoder)：**

- **作用**：将输入的文本片段转换成一个数学向量（特征嵌入），这个向量代表了文本的语义
- **具体模型**：一个标准的 **Transformer** 模型 。它使用一个大小为 49,152 的词汇表，并将文本序列的最大长度限制在 76 个词元（tokens） 

这两个编码器将图像和文本映射到一个共享的、多模态的嵌入空间中，使得相似概念的图像和文本在空间中的位置彼此靠近

### 训练过程：对比式预训练 (Contrastive Pre-training)

**对比学习**具体步骤如下:

1. **构建批次 (Batch)**：从数据集中随机抽取一批 `N` 个 (图像, 文本) 对 。例如，(图片A, "描述A")，(图片B, "描述B")，...，(图片N, "描述N")

2. **分别编码**：

   - `N`张图像被送入**图像编码器**，生成 `N` 个图像特征向量 (I1,I2,...,IN) 
   - `N` 段文本被送入**文本编码器**，生成 `N` 个文本特征向量 (T1,T2,...,TN) 

3. **计算相似度**：模型会计算这批次中**所有可能**的图像-文本对的相似度。对于 `N` 个图像和 `N` 个文本，总共有 N×N 种可能的配对组合 。相似度是通过计算图像向量和文本向量之间的**余弦相似度**来度量的 

4. **学习目标**：
   - **最大化** `N` 个**正确配对**（对角线上的 I1⋅T1, I2⋅T2 等）的余弦相似度 
   - **最小化** N2−N 个**错误配对**（所有非对角线上的组合，如 I1⋅T2, I2⋅T1 等）的余弦相似度 

### 应用：零样本预测 (Zero-Shot Prediction)

1. 拿来一张新的、需要分类的图片（例如，一张狗的图片）
2. 将这张图片送入**已经训练好的图像编码器**，生成一个图像特征向量 
3. 计算这个**图像向量**与文本经过**文本编码器**得到的**文本向量**（"A photo of a dog.", "A photo of a cat." 等）的**余弦相似度** 
4. 相似度分数最高的那个文本提示所对应的类别，就是模型的最终预测结果

## CLIP4Clip

CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval

* 论文链接：https://arxiv.org/pdf/2104.08860
* 代码仓库：https://github.com/ArrowLuo/CLIP4Clip

![](https://origin.picgo.net/2025/09/21/25-09-21-175845721249375a1b7357fd61bb0.webp)

### 核心架构组件

1. **文本编码器 (Text Encoder)** - 基本不变

这一部分与原始的 CLIP 架构完全相同

- **模型**：直接采用 CLIP 预训练好的 **Transformer 文本编码器** 

- **功能**：输入一段文字描述，输出一个代表该文字语义的**单一特征向量** (wj) 

2. **视频编码器 (Video Encoder)** - 从单帧到序列

这是第一个关键的改造点：CLIP 的图像编码器一次只能处理一张图片，而视频是连续的帧序列。

CLIP4Clip 的处理方式是：

- **模型骨干**：同样直接采用 CLIP 预训练好的**图像编码器 (ViT-B/32)** 

- **处理流程**：
	1. **帧采样**：从输入的视频中，首先采样出一系列有序的帧（即多张图片） 
  
	2. **逐帧编码**：将这些帧**一张一张地**送入 ViT 图像编码器
  
	3. **序列输出**：最终，视频编码器输出的不再是像 CLIP 那样的一个向量，而是**一个特征向量的序列** (`Zi={zi1,zi2,...,zi∣vi∣}`) 。序列中的每一个向量都代表了视频中对应帧的内容

**核心区别**：CLIP 处理图片输出**一个**向量，CLIP4Clip 处理视频输出**一串**向量

3. **相似度计算器 (Similarity Calculator)** - 核心创新

如何比较一个“文本向量”和一串“视频帧向量”，并得出一个最终的相似度分数？这是 CLIP4Clip 架构最核心的创新和研究点。论文提出了三种不同的策略来解决这个问题：

#### 策略 A: 无参数类型 (Parameter-free type)

这种方法最简单，完全依赖 CLIP 预训练好的能力，不引入任何新的学习参数 

> 当数据集较小时效果最好，因为在小型数据集上，引入新的、未初始化的参数（如序列类型中的 LSTM 或 Transformer）很难被有效训练，反而可能会损害从 CLIP 预训练模型中继承来的强大性能

- **工作方式**：
  1. 通过**平均池化 (Mean Pooling)** 将视频的所有帧特征向量聚合成一个能代表整个视频的“平均特征向量” 
  2. 计算这个“平均视频向量”和“文本向量”之间的余弦相似度，作为最终得分 
- **可以理解为**：把视频的所有精彩瞬间“平均”成一张综合的“代表图”，再用 CLIP 的方式去和文本匹配

#### 策略 B: 序列类型 (Sequential type)

这种方法认为视频帧的顺序很重要，不能简单求平均

> 当数据集足够大时，模型才有能力去学习序列类型中引入的额外参数，从而更好地捕捉视频的帧间时序关系

- **工作方式**：

  1. 在求平均之前，先将视频帧的特征序列送入一个**序列模型（如 LSTM 或 Transformer Encoder）**，这个模型会学习帧与帧之间的时间依赖关系，输出一组包含了时序信息的新特征序列
  2. 再对这组新的特征序列进行平均池化，最后计算余弦相似度 
  
- **可以理解为**：先“看完”整个视频的故事线（捕捉时序信息），形成一个整体理解，再用这个整体理解去和文本匹配

#### 策略 C: 紧密类型 (Tight type)

这种方法最为复杂，它试图让文本和视频的特征进行深度的跨模态融合

> 实际效果最差，紧密类型引入了最多的新参数来进行跨模态交互，在没有足够数据的情况下，这个模块很难被有效学习

- **工作方式**：
  1. 将**文本特征向量**和**视频帧的特征序列**直接**拼接 (concatenate)** 在一起，形成一个统一的长序列
  2. 将这个混合序列送入一个全新的 **Transformer Encoder** 中，让文本和视频的特征在内部充分交互、相互影响 
  3. 最后通过一个线性层直接预测出相似度分数
- **可以理解为**：让文本和视频的每一帧进行“对话和协商”，共同决定它们的匹配程度

## TeachCLIP

Holistic Features are almost Sufficient for Text-to-Video Retrieval

* 论文地址：https://openaccess.thecvf.com/content/CVPR2024/papers/Tian_Holistic_Features_are_almost_Sufficient_for_Text-to-Video_Retrieval_CVPR_2024_paper.pdf
* 代码仓库：https://github.com/ruc-aimc-lab/TeachCLIP?tab=readme-ov-file

![](https://origin.picgo.net/2025/09/23/25-09-23-1758558026283b04e6e9d494cbc56.webp)
