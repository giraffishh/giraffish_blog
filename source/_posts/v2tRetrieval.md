---
index_img: 'https://origin.picgo.net/2025/09/21/25-09-21-17584556830165f89cdc8dfd4716f.webp'
banner_img: 'https://img.picgo.net/2025/03/28/25-03-28-17431514678427b2138c37794ab42.webp'
title: Text-to-Video Retrieval
categories:
  - 学习笔记
tags:
  - 视频文本检索
  - 模型蒸馏
comments: true
abbrlink: 9d7503d7
date: 2025-09-22 00:55:53
updated: 2025-09-23 00:23:27
---

## CNN 卷积神经网络

图解参考：

* https://zhuanlan.zhihu.com/p/44464548
* https://cloud.tencent.com/developer/article/2109487

一个典型的 CNN 架构通常由以下几种层组合而成：

1. **输入层 (Input Layer)**
2. **卷积层 (Convolutional Layer)**
3. **激活层 (Activation Layer)**
4. **池化层 (Pooling Layer)**
5. **全连接层 (Fully Connected Layer)**
6. **输出层 (Output Layer)**

### 1. 输入层 (Input Layer)

- **作用**: 这是整个网络的起点，负责接收最原始的数据
- **功能**: 将原始图像（或其它类型数据）转换成网络可以处理的数字张量（Tensor）。对于图像而言，这个张量通常有三个维度：**[高度, 宽度, 通道数]**
  - **高度 (Height)**: 图像的像素高度
  - **宽度 (Width)**: 图像的像素宽度
  - **通道数 (Channels)**:
    - 对于灰度图，通道数为 1
    - 对于常见的彩色图（RGB），通道数为 3

### 2. 卷积层 (Convolutional Layer) - **核心**

卷积层是 CNN 的核心和灵魂，负责 **特征提取**

- **作用**: 通过一个称为“卷积核”（Kernel 或 Filter）的小窗口在输入数据上滑动，来识别并提取图像中的局部特征，例如边缘、角点、纹理、颜色块等
- **核心组件**:
  - **卷积核 (Kernel/Filter)**: 一个小型的、包含了权重参数的矩阵（例如 3x3 或 5x5）。可以把它想象成一个“模式识别器”。不同的卷积核用来检测不同的特征。例如，一个卷积核可能对垂直边缘敏感，另一个则对水平边缘敏感。网络在训练过程中会自动学习这些权重
  - **特征图 (Feature Map/Activation Map)**: 卷积核在整个输入图像上进行卷积运算后，得到的结果就是一个特征图。它表示了输入图像在特定特征上的激活程度。一张图上的哪些位置包含了这个卷积核想要寻找的模式，在特征图上对应位置的数值就越大
- **工作原理**:
  1. 卷积核在输入图像上从左到右、从上到下滑动
  2. 在每个位置，计算卷积核与输入图像对应区域的 **逐元素乘积之和**（点积运算）
  3. 将所有计算结果组合起来，形成一张新的二维矩阵，即特征图

### 3. 激活层 (Activation Layer)

- **作用**: **引入非线性因素**。这是至关重要的一步
- **功能**: 如果没有激活函数，无论 CNN 有多少层，其本质上都只是一个复杂的线性变换，无法学习和合复杂的数据模式。激活函数对卷积层输出的特征图进行非线性映射，使得网络能够学习更加复杂的特征
- **常用函数**:
  - **ReLU (Rectified Linear Unit)**: 这是目前最常用、最主流的激活函数。其公式为 f(x)=max(0,x)。
    - **优点**: 计算非常快，并且在正数区间不会出现梯度饱和（有助于缓解梯度消失问题），使得深度网络训练更容易
    - **工作方式**: 将特征图中的所有负数值都置为 0，保留正数值

### 4. 池化层 (Pooling Layer)

- **作用**: **降采样 (Downsampling)** 和 **特征降维**。
- **功能**: 对输入的特征图进行压缩，减小其空间尺寸（高度和宽度），同时保留最重要的特征信息。这样做有几个好处：
  1. **减少计算量**: 降低后续层的参数数量和计算复杂度
  2. **防止过拟合**: 通过减少特征维度来简化模型
  3. **提供平移不变性 (Translation Invariance)**: 即使特征在图像中的位置发生微小移动，池化操作后得到的结果也倾向于保持不变
- **常见类型**:
  - **最大池化 (Max Pooling)**: 在一个窗口内（如 2x2），选取数值最大的像素作为输出。它能最好地保留纹理等最显著的特征。这是最常用的一种
  - **平均池化 (Average Pooling)**: 计算窗口内所有像素的平均值作为输出。它能更好地保留背景信息

> **注意**: 池化层没有需要学习的参数。它只是一个固定的下采样操作

### 5. 全连接层 (Fully Connected Layer, FC Layer)

- **作用**: 在经过多次卷积、激活和池化操作后，将学习到的高级特征进行 **整合和分类**。
- **功能**:
  1. **扁平化 (Flatten)**: 首先，将前一层输出的多维特征图“压平”成一个一维的向量。
  2. **连接**: 该层的每个神经元都与前一层的所有神经元相连接（这也是“全连接”名称的由来）。
  3. **映射**: 通过权重矩阵的计算，将提取到的分布式特征表示映射到样本的标签空间，最终用于分类或回归。

### 6. 输出层 (Output Layer)

- **作用**: 输出最终的预测结果。
- **功能**: 它通常是一个全连接层，其神经元的数量取决于任务类型。
  - **对于多分类任务**: 神经元数量等于类别总数。通常使用 **Softmax** 激活函数，将输出值转换为表示每个类别概率的分布（所有概率值相加为 1）。
  - **对于二分类任务**: 可以使用一个神经元，并配合 **Sigmoid** 激活函数，输出一个 0 到 1 之间的概率值。
  - **对于回归任务**: 使用一个或多个神经元，并且不使用激活函数（或使用线性激活函数），直接输出预测的数值。

## CLIP

Learning Transferable Visual Models From Natural Language Supervision

* 论文链接：https://arxiv.org/pdf/2103.00020
* 代码仓库：https://github.com/OpenAI/CLIP

![](https://origin.picgo.net/2025/09/22/25-09-22-17584720224735adc63e5fb736e0c.webp)

### 核心架构组件

**图像编码器 (Image Encoder)：**

* **作用**：将输入的图像转换成一个数学向量（特征嵌入），这个向量代表了图像的内容
* **具体模型**：论文中测试了两种主流的架构 ：
  1. **ResNet-50**：一个经过改进的经典卷积神经网络（CNN）
  2. **Vision Transformer (ViT)**：一种更新的、基于 Transformer 的架构

**文本编码器 (Text Encoder)：**

- **作用**：将输入的文本片段转换成一个数学向量（特征嵌入），这个向量代表了文本的语义
- **具体模型**：一个标准的 **Transformer** 模型 。它使用一个大小为 49,152 的词汇表，并将文本序列的最大长度限制在 76 个词元（tokens） 

这两个编码器将图像和文本映射到一个共享的、多模态的嵌入空间中，使得相似概念的图像和文本在空间中的位置彼此靠近

### 训练过程：对比式预训练 (Contrastive Pre-training)

**对比学习**具体步骤如下:

1. **构建批次 (Batch)**：从数据集中随机抽取一批 `N` 个 (图像, 文本) 对 。例如，(图片A, "描述A")，(图片B, "描述B")，...，(图片N, "描述N")

2. **分别编码**：

   - `N`张图像被送入**图像编码器**，生成 `N` 个图像特征向量 (I1,I2,...,IN) 
   - `N` 段文本被送入**文本编码器**，生成 `N` 个文本特征向量 (T1,T2,...,TN) 

3. **计算相似度**：模型会计算这批次中**所有可能**的图像-文本对的相似度。对于 `N` 个图像和 `N` 个文本，总共有 N×N 种可能的配对组合 。相似度是通过计算图像向量和文本向量之间的**余弦相似度**来度量的 

4. **学习目标**：
   - **最大化** `N` 个**正确配对**（对角线上的 I1⋅T1, I2⋅T2 等）的余弦相似度 
   - **最小化** N2−N 个**错误配对**（所有非对角线上的组合，如 I1⋅T2, I2⋅T1 等）的余弦相似度 

### 应用：零样本预测 (Zero-Shot Prediction)

1. 拿来一张新的、需要分类的图片（例如，一张狗的图片）
2. 将这张图片送入**已经训练好的图像编码器**，生成一个图像特征向量 
3. 计算这个**图像向量**与文本经过**文本编码器**得到的**文本向量**（"A photo of a dog.", "A photo of a cat." 等）的**余弦相似度** 
4. 相似度分数最高的那个文本提示所对应的类别，就是模型的最终预测结果

## CLIP4Clip

CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval

* 论文链接：https://arxiv.org/pdf/2104.08860
* 代码仓库：https://github.com/ArrowLuo/CLIP4Clip

![](https://origin.picgo.net/2025/09/21/25-09-21-175845721249375a1b7357fd61bb0.webp)

### 核心架构组件

1. **文本编码器 (Text Encoder)** - 基本不变

这一部分与原始的 CLIP 架构完全相同

- **模型**：直接采用 CLIP 预训练好的 **Transformer 文本编码器** 

- **功能**：输入一段文字描述，输出一个代表该文字语义的**单一特征向量** (wj) 

2. **视频编码器 (Video Encoder)** - 从单帧到序列

这是第一个关键的改造点：CLIP 的图像编码器一次只能处理一张图片，而视频是连续的帧序列。

CLIP4Clip 的处理方式是：

- **模型骨干**：同样直接采用 CLIP 预训练好的**图像编码器 (ViT-B/32)** 

- **处理流程**：
	1. **帧采样**：从输入的视频中，首先采样出一系列有序的帧（即多张图片） 
  
	2. **逐帧编码**：将这些帧**一张一张地**送入 ViT 图像编码器
  
	3. **序列输出**：最终，视频编码器输出的不再是像 CLIP 那样的一个向量，而是**一个特征向量的序列** (`Zi={zi1,zi2,...,zi∣vi∣}`) 。序列中的每一个向量都代表了视频中对应帧的内容

**核心区别**：CLIP 处理图片输出**一个**向量，CLIP4Clip 处理视频输出**一串**向量

3. **相似度计算器 (Similarity Calculator)** - 核心创新

如何比较一个“文本向量”和一串“视频帧向量”，并得出一个最终的相似度分数？这是 CLIP4Clip 架构最核心的创新和研究点。论文提出了三种不同的策略来解决这个问题：

#### 策略 A: 无参数类型 (Parameter-free type)

这种方法最简单，完全依赖 CLIP 预训练好的能力，不引入任何新的学习参数 

> 当数据集较小时效果最好，因为在小型数据集上，引入新的、未初始化的参数（如序列类型中的 LSTM 或 Transformer）很难被有效训练，反而可能会损害从 CLIP 预训练模型中继承来的强大性能

- **工作方式**：
  1. 通过**平均池化 (Mean Pooling)** 将视频的所有帧特征向量聚合成一个能代表整个视频的“平均特征向量” 
  2. 计算这个“平均视频向量”和“文本向量”之间的余弦相似度，作为最终得分 
- **可以理解为**：把视频的所有精彩瞬间“平均”成一张综合的“代表图”，再用 CLIP 的方式去和文本匹配

#### 策略 B: 序列类型 (Sequential type)

这种方法认为视频帧的顺序很重要，不能简单求平均

> 当数据集足够大时，模型才有能力去学习序列类型中引入的额外参数，从而更好地捕捉视频的帧间时序关系

- **工作方式**：

  1. 在求平均之前，先将视频帧的特征序列送入一个**序列模型（如 LSTM 或 Transformer Encoder）**，这个模型会学习帧与帧之间的时间依赖关系，输出一组包含了时序信息的新特征序列
  2. 再对这组新的特征序列进行平均池化，最后计算余弦相似度 
  
- **可以理解为**：先“看完”整个视频的故事线（捕捉时序信息），形成一个整体理解，再用这个整体理解去和文本匹配

#### 策略 C: 紧密类型 (Tight type)

这种方法最为复杂，它试图让文本和视频的特征进行深度的跨模态融合

> 实际效果最差，紧密类型引入了最多的新参数来进行跨模态交互，在没有足够数据的情况下，这个模块很难被有效学习

- **工作方式**：
  1. 将**文本特征向量**和**视频帧的特征序列**直接**拼接 (concatenate)** 在一起，形成一个统一的长序列
  2. 将这个混合序列送入一个全新的 **Transformer Encoder** 中，让文本和视频的特征在内部充分交互、相互影响 
  3. 最后通过一个线性层直接预测出相似度分数
- **可以理解为**：让文本和视频的每一帧进行“对话和协商”，共同决定它们的匹配程度

## TeachCLIP

Holistic Features are almost Sufficient for Text-to-Video Retrieval

* 论文地址：https://openaccess.thecvf.com/content/CVPR2024/papers/Tian_Holistic_Features_are_almost_Sufficient_for_Text-to-Video_Retrieval_CVPR_2024_paper.pdf
* 代码仓库：https://github.com/ruc-aimc-lab/TeachCLIP?tab=readme-ov-file

![](https://origin.picgo.net/2025/09/23/25-09-23-1758558026283b04e6e9d494cbc56.webp)
