---
index_img: 'https://cdn.giraffish.me/blog/25-05-31-1748685017800.webp'
banner_img: 'https://cdn.giraffish.me/blog/25-05-31-1748684560329.webp'
title: 数模笔记模型篇二
categories:
  - 学习笔记
tags:
  - 数学建模
comments: true
abbrlink: c8b9524f
date: 2025-07-18 16:36:56
updated: 2025-07-18 16:36:56
---
## 灰色预测 GM(1,1) 

灰色系统理论旨在解决**小样本**、**贫信息**下的不确定性问题。它不追求数据的统计规律，而是通过特定方法处理原始数据，寻找其内在的发展趋势

GM(1,1)是该理论中最基础的模型，其名称含义为：**G**rey **M**odel of **First-order** equation in **One** variable（一阶单变量灰色模型）

其核心操作是**累加生成(AGO)**，即将原始的、可能看似杂乱的数据序列进行累加，生成一个更平滑、趋势性更强的序列，然后对这个新序列建模

为了直观理解，可以将数据预测过程比作观察并预测一棵树的生长：

* **原始数据 $X^{(0)}$**: 你**每天**测量的树的生长量（例如：2cm, 3cm, 2.5cm...）。数据波动较大
* **累加数据 $X^{(1)}$**: 树的**累计总高度**（例如：2cm, 5cm, 7.5cm...）。这条曲线比每日生长量平滑得多
* **GM(1,1)的目标**: 通过对相对规律的“总高度”曲线进行建模，来反向推断并预测未来不规律的“每日生长量”


### 核心步骤

假设原始非负数据序列为 $X^{(0)} = (x^{(0)}(1), x^{(0)}(2), \dots, x^{(0)}(n))$

#### 1. 累加生成 (AGO)

**目的**：削弱原始数据的随机性，使其呈现单调趋势
对 $X^{(0)}$ 进行一次累加，得到一阶累加生成序列 $X^{(1)}$

$$
x^{(1)}(k) = \sum_{i=1}^{k} x^{(0)}(i)
$$

* $x^{(1)}(1) = x^{(0)}(1)$
* $x^{(1)}(2) = x^{(0)}(1) + x^{(0)}(2)$
* ...

#### 2. 构造背景值 (紧邻均值生成)

**目的**：为建立微分方程模型做准备，用$X^{(1)}$的均值来近似其积分
构造背景值序列 $Z^{(1)}$

$$
z^{(1)}(k) = 0.5 (x^{(1)}(k) + x^{(1)}(k-1)), \quad k = 2, 3, \dots, n
$$

#### 3. 建立灰色微分方程并求解参数

**目的**：从离散的数据点中，提炼出能代表整体趋势的核心参数 `$a$` 和 `$b$`
**灰色微分方程**:
$$
x^{(0)}(k) + a z^{(1)}(k) = b
$$

**参数含义**:
* $a$: **发展系数 (Development Coefficient)**。决定了趋势发展的强度
* $b$: **灰色作用量 (Grey Action Quantity)**。反映了数据的内生驱动力

**求解**: 使用最小二乘法求解参数列向量 $\hat{u} = [a, b]^T$

$$
Y = \begin{pmatrix} x^{(0)}(2) \\ x^{(0)}(3) \\ \vdots \\ x^{(0)}(n) \end{pmatrix}, \quad
B = \begin{pmatrix} -z^{(1)}(2) & 1 \\ -z^{(1)}(3) & 1 \\ \vdots & \vdots \\ -z^{(1)}(n) & 1 \end{pmatrix}
$$

$$
\hat{u} = (B^T B)^{-1} B^T Y
$$

#### 4. 建立预测模型 (白化方程)

**目的**：将离散的灰色关系转化为连续的、可求解的微分方程，从而得到一个可以预测未来的“生长曲线”公式

**白化方程 (时间的连续函数)**:
$$
\frac{dx^{(1)}}{dt} + a x^{(1)} = b
$$

**求解方程得到预测公式**:
$$
\hat{x}^{(1)}(k+1) = \left(x^{(0)}(1) - \frac{b}{a}\right) e^{-ak} + \frac{b}{a}
$$

> 该公式用于预测**累加序列**在未来任意时刻的值

#### 5. 累减还原 (IAGO)

**目的**：将预测出的“累计总高度”还原为我们真正关心的“每日生长量”
对预测出的 $\hat{X}^{(1)}$ 序列进行逆向操作

* $\hat{x}^{(0)}(1) = x^{(0)}(1)$
* $\hat{x}^{(0)}(k+1) = \hat{x}^{(1)}(k+1) - \hat{x}^{(1)}(k)$

这样就得到了原始序列的拟合值与预测值

### 模型检验

| 检验类型 | 检验对象 | 检验目的 | 检验性质 |
| :--- | :--- | :--- | :--- |
| **级比偏差检验** | 模型参数 `a` 决定的理论趋势 与 数据自身的演化趋势 | 评估模型的**内部结构合理性** | **内部结构**检验 |
| **残差/后验差检验** | 最终预测值 $\hat{x}^{(0)}$与 原始真实值 $x^{(0)}$ | 评估模型的**最终预测精度** | **外部精度**检验 |

#### 1. 级比偏差检验

1. 计算单个级比偏差 $\rho(k)$
$$
\rho(k) = \left| 1 - \left(\frac{1-0.5a}{1+0.5a}\right) \frac{1}{\sigma(k)} \right|
$$

-   $a$为模型已求出的发展系数
-   $\sigma(k) = \frac{x^{(0)}(k-1)}{x^{(0)}(k)}$为原始数据的级比

2. 计算平均级比偏差 $\bar{\rho}$
$$
\bar{\rho} = \frac{1}{n-1} \sum_{k=2}^{n} \rho(k)
$$

**评判标准：**

| 平均级比偏差 `$\bar{\rho}$` | 级别 |
|:-------------------:|:----:|
| $< 0.1$             |  优  |
| $< 0.2$             | 合格 |
| $\ge 0.2$           | 不合格 |

#### 2. 残差检验

**目的**：直观判断模型对历史数据的拟合程度

* **相对误差**:
$$
\Delta_k = \left| \frac{x^{(0)}(k) - \hat{x}^{(0)}(k)}{x^{(0)}(k)} \right|
$$

* **平均相对精度**:
$$
P_{\text{精度}} = \left(1 - \frac{1}{n}\sum_{k=1}^{n} \Delta_k\right) \times 100\%
$$

| 平均相对精度 | 级别 |
|:----------:|:----:|
| > 90%      |  优  |
| > 80%      |  良  |
| > 70%      | 合格 |
| < 70%      | 不合格 |

#### 3. 后验差检验
**目的**：更严格地从统计角度评估模型的泛化能力

1.  计算原始序列方差 $S_1^2$ 和残差序列方差 $S_2^2$
2.  计算**后验差比值 C**:
$$
C = \frac{S_2}{S_1}
$$
3.  计算**小误差概率 P**:
$$
P = \text{Prob} (|\text{残差}(k) - \text{残差均值}| < 0.6745 S_1)
$$

根据C值和P值，将模型精度分为四个等级：

| 精度等级 | C 值 | P 值 |
|:----------:|:----------:|:----------:|
| **优 (一级)** | $C \le 0.35$ | $P \ge 0.95$ |
| **合格 (二级)** | $C \le 0.50$ | $P \ge 0.80$ |
| **勉强合格 (三级)** | $C \le 0.65$ | $P \ge 0.70$ |
| **不合格 (四级)** | $C > 0.65$ | $P < 0.70$ |

**只有通过后验差检验（至少达到勉强合格）的模型，才能用于外推预测**

### 优缺点与适用场景

**优点**
* **样本需求少**: 理论上4个数据点即可建模，非常适合数据稀疏的场景
* **分布要求宽**: 不需要数据服从特定的统计分布（如正态分布）
* **计算简单**: 相较于复杂的计量或机器学习模型，计算过程清晰、快捷
* **短期预测准**: 对于具有明显单调（类指数）趋势的序列，短期预测效果好

**缺点**
* **适用范围窄**: 对波动性强、周期性或无明显趋势的序列效果很差
* **中长期预测差**: 本质是指数拟合，预测期越长，误差累积越严重
* **模型较敏感**: 原始数据的微小变动或新数据的加入可能导致模型参数变化较大

**适用场景**
适用于**数据量有限**的**短期**、**单调趋势**预测

* **社会经济**: 能源消耗、粮食产量、人口数量、特定商品价格的短期趋势
* **管理决策**: 新产品销量、网站访问量、库存变化的短期预测
* **地质水文**: 河流径流量、自然灾害发生次数的短期预测

### 示例代码

```python
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


class GreyForecast:
    """
    GM(1,1)灰色预测模型
    该类实现了灰色预测的完整流程，包括级比检验、建模、预测和多维度检验。
    """

    def __init__(self, data, n_preds):
        """
        初始化模型
        :param data: 原始数据序列 (list or numpy array)
        :param n_preds: 需要向后预测的期数 (int)
        """
        if not isinstance(data, (list, np.ndarray)) or len(data) < 4:
            raise ValueError("输入数据必须是列表或Numpy数组，且至少包含4个数据点。")
        self.original_data = np.array(data)
        self.n_preds = n_preds
        self.n = len(data)
        self.prediction_results = None

    def _level_ratio_test(self, data_series):
        """
        执行级比检验
        :param data_series: 需要检验的数据序列
        :return: (bool, list) 是否通过检验，级比序列
        """
        level_ratios = data_series[1:] / data_series[:-1]
        lower_bound = np.exp(-2 / (self.n + 1))
        upper_bound = np.exp(2 / (self.n + 1))

        passed = np.all((level_ratios > lower_bound) & (level_ratios < upper_bound))
        return passed, level_ratios

    def fit(self):
        """
        执行建模与预测全流程
        """
        x0 = self.original_data
        translation_c = 0

        # 1. 建模前级比检验
        test_passed, _ = self._level_ratio_test(x0)

        if not test_passed:
            # 尝试平移变换
            translation_c = -x0.min() + 1  # 保证所有数据为正
            x0 = x0 + translation_c
            test_passed_after_translation, _ = self._level_ratio_test(x0)
            if not test_passed_after_translation:
                print("警告：原始数据未通过级比检验，且平移变换后仍未通过。模型可能不适用。")

        # 2. 累加生成 (AGO)
        x1 = np.cumsum(x0)

        # 3. 构造背景值 (紧邻均值生成)
        z1 = 0.5 * (x1[:-1] + x1[1:])

        # 4. 最小二乘法求解参数 a, b
        Y = x0[1:].reshape(-1, 1)
        B = np.vstack((-z1, np.ones_like(z1))).T

        # u = [a, b]^T
        u = np.linalg.inv(B.T @ B) @ B.T @ Y
        a, b = u.flatten()

        # 5. 建立预测模型
        total_len = self.n + self.n_preds
        x1_fit_pred = np.zeros(total_len)
        x1_fit_pred[0] = x0[0]

        for k in range(1, total_len):
            x1_fit_pred[k] = (x0[0] - b / a) * np.exp(-a * k) + b / a

        # 6. 累减还原 (IAGO)
        x0_fit_pred = np.zeros(total_len)
        x0_fit_pred[0] = x0[0]
        x0_fit_pred[1:] = x1_fit_pred[1:] - x1_fit_pred[:-1]

        # 7. 如果进行了平移，进行逆变换
        if translation_c > 0:
            x0_fit_pred -= translation_c

        # 8. 分离拟合值和预测值
        fitted_values = x0_fit_pred[:self.n]
        predicted_values = x0_fit_pred[self.n:]

        # 9. 模型检验
        tests = self._run_post_tests(self.original_data, fitted_values, a)

        self.prediction_results = {
            "parameters": {"a": a, "b": b},
            "translation_c": translation_c,
            "fitted_values": fitted_values,
            "predicted_values": predicted_values,
            "tests": tests
        }
        return self.prediction_results

    def _run_post_tests(self, original, fitted, a):
        """
        执行所有建模后的检验
        """
        # --- 残差检验 ---
        residuals = original - fitted
        relative_errors = np.abs(residuals / original)
        mean_relative_error = np.mean(relative_errors)
        accuracy = 1 - mean_relative_error

        # --- 后验差检验 ---
        s1_sq = np.var(original, ddof=1)
        s2_sq = np.var(residuals, ddof=1)
        C = np.sqrt(s2_sq / s1_sq)

        # 小误差概率P
        s1 = np.std(original, ddof=1)
        p_count = np.sum(np.abs(residuals - np.mean(residuals)) < 0.6745 * s1)
        P = p_count / self.n

        # --- 级比偏差检验 ---
        _, level_ratios = self._level_ratio_test(self.original_data)  # 使用原始数据级比
        theory_ratio = (1 - 0.5 * a) / (1 + 0.5 * a)
        ratio_deviations = np.abs(1 - theory_ratio / level_ratios[1:])  # sigma(k) from k=2
        mean_ratio_deviation = np.mean(ratio_deviations)

        return {
            "residual_test": {
                "residuals": residuals,
                "relative_errors": relative_errors,
                "mean_relative_error": mean_relative_error,
                "accuracy": accuracy
            },
            "posterior_error_test": {
                "C_ratio": C,
                "P_small_error_prob": P
            },
            "level_ratio_deviation_test": {
                "mean_deviation": mean_ratio_deviation
            }
        }

    def plot(self):
        """
        绘制结果图
        """
        if self.prediction_results is None:
            print("请先调用 .fit() 方法进行建模。")
            return

        original_time_axis = np.arange(1, self.n + 1)
        fitted_time_axis = np.arange(1, self.n + 1)
        predicted_time_axis = np.arange(self.n + 1, self.n + self.n_preds + 1)

        plt.figure(figsize=(12, 7))
        plt.plot(original_time_axis, self.original_data, 'o-', label='原始数据 (Original Data)', color='blue')
        plt.plot(fitted_time_axis, self.prediction_results['fitted_values'], 's--', label='拟合数据 (Fitted Data)',
                 color='green')
        plt.plot(predicted_time_axis, self.prediction_results['predicted_values'], '^-',
                 label='预测数据 (Predicted Data)', color='red')

        plt.title('GM(1,1) 灰色预测模型结果')
        plt.xlabel('时间序列 (Time Series)')
        plt.ylabel('数值 (Value)')
        plt.legend()
        plt.grid(True, linestyle='--', alpha=0.6)
        plt.show()

    def print_results(self):
        """
        格式化打印所有结果
        """
        if self.prediction_results is None:
            print("请先调用 .fit() 方法进行建模。")
            return

        res = self.prediction_results
        print("\n" + "=" * 50)
        print("GM(1,1) 灰色预测模型结果报告")
        print("=" * 50)

        # 添加级比检验结果打印
        print("\n[1] 级比检验结果")
        original_passed, original_ratios = self._level_ratio_test(self.original_data)
        lower_bound = np.exp(-2 / (self.n + 1))
        upper_bound = np.exp(2 / (self.n + 1))

        print(f"  - 级比检验区间: ({lower_bound:.4f}, {upper_bound:.4f})")
        print(f"  - 原始数据级比检验: {'通过' if original_passed else '不通过'}")
        print(f"  - 级比序列: {[f'{ratio:.4f}' for ratio in original_ratios]}")

        if res['translation_c'] > 0:
            transformed_data = self.original_data + res['translation_c']
            transformed_passed, transformed_ratios = self._level_ratio_test(transformed_data)
            print(f"  - 平移变换后级比检验: {'通过' if transformed_passed else '不通过'}")
            print(f"  - 平移后级比序列: {[f'{ratio:.4f}' for ratio in transformed_ratios]}")

        print("\n[2] 模型参数")
        print(f"  - 发展系数 a = {res['parameters']['a']:.4f}")
        print(f"  - 灰色作用量 b = {res['parameters']['b']:.4f}")
        if res['translation_c'] > 0:
            print(f"  - 平移变换常数 c = {res['translation_c']:.4f}")

        print("\n[3] 拟合与预测值")
        for i, val in enumerate(res['fitted_values']):
            print(f"  - 第 {i + 1} 期 (拟合值): {val:.4f}")
        for i, val in enumerate(res['predicted_values']):
            print(f"  - 第 {self.n + i + 1} 期 (预测值): {val:.4f}")

        print("\n[4] 模型检验结果")
        # 残差检验
        acc = res['tests']['residual_test']['accuracy']
        print(f"\n--- 4.1 残差检验 ---")
        print(f"  - 平均相对误差: {1 - acc:.4%}")
        print(f"  - 模型精度: {acc:.4%}")

        # 后验差检验
        C = res['tests']['posterior_error_test']['C_ratio']
        P = res['tests']['posterior_error_test']['P_small_error_prob']
        c_level = "优" if C <= 0.35 else "合格" if C <= 0.50 else "勉强合格" if C <= 0.65 else "不合格"
        p_level = "优" if P >= 0.95 else "合格" if P >= 0.80 else "勉强合格" if P >= 0.70 else "不合格"
        print(f"\n--- 4.2 后验差检验 ---")
        print(f"  - 后验差比值 C = {C:.4f} (等级: {c_level})")
        print(f"  - 小误差概率 P = {P:.4f} (等级: {p_level})")

        # 级比偏差检验
        rho = res['tests']['level_ratio_deviation_test']['mean_deviation']
        rho_level = "优" if rho < 0.1 else "合格" if rho < 0.2 else "不合格"
        print(f"\n--- 4.3 级比偏差检验 ---")
        print(f"  - 平均级比偏差 = {rho:.4f} (等级: {rho_level})")

        print("\n" + "=" * 50)


# ==============================================================================
#                                  示例用法
# ==============================================================================
if __name__ == '__main__':
    # 示例1：一组典型的增长数据
    print("--- 示例 1：典型增长数据 ---")
    # 数据来源：某地区财政收入，单位：亿元
    raw_data_1 = [71.8, 80.6, 96.5, 108.3, 118.9, 130.1]
    # 建立模型，向后预测3期
    gm_model_1 = GreyForecast(raw_data_1, n_preds=3)
    # 执行建模
    gm_model_1.fit()
    # 打印结果报告
    gm_model_1.print_results()
    # 绘制图表
    gm_model_1.plot()

    # 示例2：一组波动较大，可能需要平移的数据
    print("\n\n--- 示例 2：波动较大，不一定适用的示例 ---")
    raw_data_2 = [22, 20, 25, 28, 26, 30, 34, 32]
    # 建立模型，向后预测4期
    gm_model_2 = GreyForecast(raw_data_2, n_preds=4)
    gm_model_2.fit()
    gm_model_2.print_results()
    gm_model_2.plot()
```

## ARIMA 时间序列模型

ARIMA模型，全称为**差分整合移动平均自回归模型 (Autoregressive Integrated Moving Average Model)**，是应用统计学中进行时间序列分析和预测的基石模型。它的核心思想是，一个时间序列中的数值变化规律可以通过其自身过去的值、过去的预测误差以及这两者的线性组合来捕捉和描述

该模型由三个核心参数`(p, d, q)`定义，每个字母都代表了模型的一个关键组成部分：

* **p (Autoregressive - AR)**: 自回归阶数，指当前值与过去多少个**观测值**直接相关
* **d (Integrated - I)**: 差分阶数，指为使序列变得**平稳**所需要进行的差分运算次数
* **q (Moving Average - MA)**: 移动平均阶数，指当前值与过去多少个**预测误差**直接相关

### 平稳性 (Stationarity)与差分

平稳性是ARIMA模型应用的先决条件。一个平稳的时间序列，其统计特性不随时间的推移而改变。具体来说，它应满足：
1.  **均值恒定**: 序列的平均水平不随时间改变
2.  **方差恒定**: 序列的波动幅度不随时间改变（无异方差）
3.  **自协方差不依赖于时间**: 序列在任意两个时间点`t`和`t-k`的关联性只取决于时间间隔`k`，而与`t`的位置无关

> 只有当序列的统计规律不随时间变化时，我们才能相信从历史数据中学到的模式（如均值、方差、自相关性）在未来同样适用，从而使预测成为可能

**差分**是处理非平稳性的主要手段，其操作由参数`d`即**差分次数**控制，具体为计算相邻观测值之差，一阶差分$Y'_t = Y_t - Y_{t-1}$通常能消除线性趋势

判断是否平稳的方法：
1. 视觉判断: 观察时序图，如果存在明显趋势或季节性，则序列非平稳。
2. 统计检验: 使用**增强迪基-福勒检验 (Augmented Dickey-Fuller Test, ADF检验)**
    * **原假设 (H₀)**: 序列存在单位根，即**非平稳**
    * **检验目标**: 我们希望检验结果的**p值小于0.05**，这样就可以拒绝原假设，认为序列是平稳的

> 即对原始序列进行ADF检验。若不平稳，则进行差分，再对差分后的序列进行检验。重复此过程，直到检验通过。总共的差分次数就是`d`的值，`d`通常不超过2


### 模型构成

在序列通过差分变得平稳后，我们用AR和MA部分来对其中的动态结构进行建模

#### AR($p$) - 自回归部分

描述当前值与过去值之间的线性关系，即“系统的记忆”

**数学形式**:
$$
Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2} + \dots + \phi_p Y_{t-p} + \epsilon_t
$$

>其中 $\phi$ 是自回归系数，$p$ 是阶数

**偏自相关函数图 (PACF)**：衡量了在剔除中间所有滞后项的传递影响后，$Y_t$与$Y_{t-k}$之间纯粹的、直接的相关性
对于一个纯AR($p$)过程，其PACF图会在**滞后第$p$阶后出现清晰的截尾**（相关系数突然跌落至置信区间内并保持在其中）

#### MA($q$) - 移动平均部分

描述当前值与过去预测误差（随机冲击）之间的关系，即“冲击的记忆”。它**不是**指滚动的平均值

**数学形式**:

$$
Y_t = \mu + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + \dots + \theta_q \epsilon_{t-q}
$$

> 其中 $\theta$ 是移动平均系数，$q$ 是阶数

**自相关函数图 (ACF)**：衡量了$Y_t$与$Y_{t-k}$之间所有的（直接和间接的）相关性
对于一个纯MA($q$)过程，其ACF图会在**滞后第$q$阶后出现清晰的截尾**

### 核心步骤

Box-Jenkins方法论是一个由四个步骤构成的、科学且迭代的建模框架

#### 1. 模型识别 (Identification)

确定最合适的模型阶数$(p, d, q)$

1.  **定 $d$**: 通过ADF检验确定差分次数
2.  **定 $p$, $q$**: 对差分后的平稳序列，绘制ACF和PACF图
    * **PACF截尾，ACF拖尾** -> 考虑AR($p$)模型，即ARIMA$(p, d, 0)$
    * **ACF截尾，PACF拖尾** -> 考虑MA($q$)模型，即ARIMA$(0, d, q)$
    * **ACF和PACF均拖尾** -> 考虑ARMA($p,q$)模型，即ARIMA$(p, d, q)$

**信息准则辅助**: 当图形特征不明显时，使用**AIC（赤池信息量准则）**或**BIC（贝叶斯信息量准则）**，但是需要在模型的拟合优度与复杂度之间做权衡，我们倾向于选择AIC或BIC值**最小**的模型，BIC对模型复杂度的惩罚更重，因此更倾向于推荐简约模型

> 一般BIC效果更好，但是不能盲目根据AIC或BIC的结果之间确定模型，还应结合PACF和ACF图形进行判断

#### 2. 参数估计 (Estimation)

在确定了模型结构$(p,d,q)$后，计算出所有未知参数（$\phi$, $\theta$, $c$等）的具体数值

**最大似然估计 (Maximum Likelihood Estimation, MLE)**

假定模型的误差项$\epsilon_t$服从正态分布，MLE会寻找一组能使我们观测到的这组真实数据出现的概率（似然）达到最大的参数值，这是一个由计算机执行的复杂数值优化过程

#### 3. 模型诊断 (Diagnostic Checking)

对已估计好的模型进行“质检”，确保其充分捕捉了数据信息，通过检验模型的**残差 (Residuals =真实值 - 预测值)** 是否满足**白噪声**的假定

**诊断方法：**

1. **残差时序图**: 检查残差是否围绕0随机波动，且方差恒定
2. **残差ACF图**: 检查残差是否存在自相关。理想情况下，所有滞后阶数（lag>0）的柱子都应在置信区间内
3. **Ljung-Box Q检验**: 对残差的自相关性进行正式的统计检验
    * **原假设($H_0$)**: 残差序列不存在自相关性
    * **判决**: 我们希望检验的**p值 > 0.05**。一个大的p值意味着模型通过检验，残差是随机的
4. **残差直方图/Q-Q图**: 检查残差是否服从正态分布，这对于预测区间的可靠性至关重要

> **迭代循环**: 若诊断检验失败（如Ljung-Box检验p值很小），必须返回第一步，重新识别模型（例如调整$p$或$q$的值），然后再次进行估计和诊断，直至模型通过检验

#### 4. 模型应用与预测 (Application & Forecasting)

使用通过所有检验的最终模型来预测未来的数值，预测结果通常包含一个**点预测**（最可能的值）和一个**预测区间**（一个值范围）

> 随着预测时间向未来延伸，不确定性会累积，导致预测区间越来越宽。

### ARIMA的局限与扩展模型

#### 模型局限性
* **线性关系**: 只能捕捉线性规律，对于复杂的非线性模式无能为力
* **单变量**: 传统ARIMA无法整合外部信息（如天气、政策、其他经济指标等）
* **常数方差**: 假设误差的波动性是恒定的，不适用于金融市场等波动性聚集的场景

#### 扩展模型

1. **ARIMAX (ARIMA with eXogenous variables)**: 在ARIMA模型中加入了外生解释变量，使得模型可以利用外部信息进行预测
2. GARCH族模型**: 用于处理时变的波动性（异方差），常与ARIMA模型结合使用，先用ARIMA对收益率建模，再用GARCH对残差的波动性建模
3. SARIMA (Seasonal ARIMA)**: ARIMA$(p,d,q)(P,D,Q)_m$，在ARIMA基础上增加了季节性参数，专门用于处理具有固定周期（如年度、季度）的数据

**SARIMA:**

基础ARIMA模型难以处理具有明显**季节性（Seasonality）**规律的时间序列。季节性指的是数据中以固定频率（如每年、每季度、每周）重复出现的模式。例如，零售业销售额通常在每年年底出现高峰

SARIMA模型的核心思想可以理解为**两个ARIMA模型的巧妙结合**：一个用于描述短期非季节性动态，另一个用于描述长期季节性动态

它引入了四个新的**季节性参数**：
* **$m$ (Seasonal Period)**: **季节性周期长度**。这是最重要的参数，它定义了“季节”的长度。对于月度数据，$m=12$；对于季度数据，$m=4$
* **$P$ (Seasonal AR Order)**: **季节性自回归阶数**。表示当前值与过去季节的对应值（如$Y_{t-m}$, $Y_{t-2m}$）之间的关系
* **$D$ (Seasonal Differencing Order)**: **季节性差分阶数**。用于消除季节性趋势。其操作为：$Y'_t = Y_t - Y_{t-m}$。例如，用今年1月的数据减去去年1月的数据
* **$Q$ (Seasonal MA Order)**: **季节性移动平均阶数**。表示当前值与过去季节的预测误差（如$\epsilon_{t-m}$, $\epsilon_{t-2m}$）之间的关系

一个完整的SARIMA模型表示为：

$$
\text{SARIMA}(p,d,q)(P,D,Q)_m
$$

建模过程与Box-Jenkins方法论类似，但在识别阶段需要特别关注：
1. 观察原始序列的ACF图，如果在大间隔的滞后（如$m, 2m, 3m, \dots$）上有显著的峰值，则强烈暗示存在季节性
2. 先进行季节性差分（定$D$），再进行普通差分（定$d$），直至序列平稳
3. 在平稳序列的ACF/PACF图上，观察滞后$m, 2m, \dots$处的模式来确定$P$和$Q$，同时观察短期滞后（1, 2, 3, ...）处的模式来确定$p$和$q$

### 代码示例

```python
# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pmdarima as pm

import warnings

# This will specifically ignore the FutureWarning related to 'force_all_finite'
warnings.filterwarnings('ignore', category=FutureWarning)

# 全局设置，用于解决Matplotlib中文显示问题
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False


# --- 步骤 1: 数据加载与准备 ---

# 为了代码的可运行性，我们首先创建一个模拟数据集。
def create_sample_data():
    """
    创建一个带线性趋势和标准年度（12个月）季节性的模拟时间序列数据。
    """
    # 设置随机种子以保证结果可复现
    np.random.seed(42)

    # 10年的月度数据，共120个样本
    n_samples = 120

    # 创建日期范围 (2010-01-01 到 2019-12-01)
    dates = pd.date_range(start='2010-01-01', periods=n_samples, freq='MS')

    # 1. 创建线性趋势项
    trend = np.linspace(0, 50, n_samples)

    # 2. 创建季节性项 (核心修正)
    # 为了在10年(120个月)内有10个完整的周期，角度需要从0变化到 10 * 2 * pi
    n_years = 10
    seasonal = 15 * np.sin(np.linspace(0, n_years * 2 * np.pi, n_samples))

    # 3. 创建随机噪声项
    noise = np.random.normal(0, 5, n_samples)

    # 4. 合成数据 (趋势 + 季节性 + 噪声 + 基础值)
    data = 50 + trend + seasonal + noise

    # 创建一个带日期索引的Pandas Series，这更符合时间序列分析的习惯
    ts_data = pd.Series(data, index=dates, name='value')

    return ts_data

# 使用模拟数据
ts_data = create_sample_data()

# 【竞赛中请替换为您的数据加载代码】
# 例如:
# df = pd.read_csv('您的数据.csv')
# df['date_column'] = pd.to_datetime(df['date_column']) # 转换日期列
# df = df.set_index('date_column') # 将日期列设为索引
# ts_data = df['your_value_column']

print("--- 数据预览 ---")
print(ts_data.head())
print("\n")

# 可视化原始数据
plt.figure(figsize=(12, 6))
plt.plot(ts_data, label='原始数据')
plt.title('原始时间序列图')
plt.xlabel('日期')
plt.ylabel('数值')
plt.legend()
plt.grid(True)
plt.show()


# --- 步骤 2: 平稳性检验 (确定差分阶数d) ---

def adf_test(timeseries):
    """
    执行ADF检验并打印结果。
    ADF检验的原假设(H0)是：序列存在单位根，即非平稳。
    如果p-value < 0.05，我们拒绝原假设，认为序列是平稳的。
    """
    print('--- ADF检验结果 ---')
    # adfuller函数返回多个值，我们主要关心第一个（ADF统计量）和第二个（p-value）
    result = adfuller(timeseries, autolag='AIC')
    print(f'ADF 统计量: {result[0]}')
    print(f'p-value: {result[1]}')
    print('临界值:')
    for key, value in result[4].items():
        print(f'\t{key}: {value}')

    if result[1] <= 0.05:
        print("结论: 序列是平稳的 (拒绝原假设)")
    else:
        print("结论: 序列是非平稳的 (无法拒绝原假设)")


print("--- 对原始序列进行平稳性检验 ---")
adf_test(ts_data)
print("\n")

# --- 步骤 2.5: ACF和PACF图 (用于模型识别) ---

# ACF和PACF图是识别ARIMA模型参数的重要工具
# ACF(自相关函数)用于识别MA(q)参数
# PACF(偏自相关函数)用于识别AR(p)参数

print("--- 绘制ACF和PACF图进行模型识别 ---")

# 创建子图 - 改为3行2列布局
fig, axes = plt.subplots(3, 2, figsize=(15, 15))

# 原始序列的ACF和PACF
plot_acf(ts_data, ax=axes[0, 0], lags=40, title='原始序列 - 自相关函数(ACF)')
plot_pacf(ts_data, ax=axes[0, 1], lags=40, title='原始序列 - 偏自相关函数(PACF)')

# 一阶差分序列的ACF和PACF
ts_diff = ts_data.diff().dropna()
plot_acf(ts_diff, ax=axes[1, 0], lags=40, title='一阶差分序列 - 自相关函数(ACF)')
plot_pacf(ts_diff, ax=axes[1, 1], lags=40, title='一阶差分序列 - 偏自相关函数(PACF)')

# 二阶差分序列的ACF和PACF
ts_diff2 = ts_data.diff().diff().dropna()
plot_acf(ts_diff2, ax=axes[2, 0], lags=40, title='二阶差分序列 - 自相关函数(ACF)')
plot_pacf(ts_diff2, ax=axes[2, 1], lags=40, title='二阶差分序列 - 偏自相关函数(PACF)')

plt.tight_layout()
plt.show()

# 对一阶差分序列进行平稳性检验
print("--- 对一阶差分序列进行平稳性检验 ---")
adf_test(ts_diff)
print("\n")

# 对二阶差分序列进行平稳性检验
print("--- 对二阶差分序列进行平稳性检验 ---")
adf_test(ts_diff2)
print("\n")

# --- 步骤 3: 模型识别与拟合 (自动调参) ---

# 使用auto_arima自动模型选择
print("--- 使用auto_arima自动寻找最优模型 ---")
model_fit = pm.auto_arima(ts_data,
                           start_p=1, start_q=1,
                           test='adf',  # 使用adf检验来确定d
                           max_p=8, max_q=8,  # p和q的最大值
                           m=12,  # 季节性周期
                           d=1,  #  差分阶数，None表示自动选择
                           seasonal=True,  # 开启季节性
                           start_P=0,
                           D=None,  # 季节性差分阶数，None表示自动选择
                           trace=True,  # 打印尝试的每个模型的信息
                           error_action='ignore',
                           suppress_warnings=True,
                           stepwise=True,   # 逐步搜索，速度更快，但会陷入局部最优解
                           information_criterion='bic')

# 注释掉手动指定的SARIMAX模型
# print("--- 使用手动指定的SARIMAX模型 ---")
#
# # 创建SARIMAX模型
# model = SARIMAX(ts_data,
#                       order=(0, 1, 1),
#                       seasonal_order=(0, 1, 1, 30),
#                       enforce_stationarity=False,
#                       enforce_invertibility=False)

# auto_arima返回的是已经拟合好的模型，不需要再调用fit()
# model_fit = model.fit(disp=False)

print("\n--- 自动选择的最优模型摘要 ---")
print(model_fit.summary())
print("\n")

# --- 步骤 4: 模型诊断 (检验残差是否为白噪声) ---

# 模型诊断是至关重要的一步，用于检验模型的充分性。
# 一个好的模型，其残差应该像白噪声，即没有自相关性。
print("--- 模型诊断 ---")
# plot_diagnostics会生成四张图，全方位诊断模型残差
# 1. 标准化残差时序图: 残差应围绕0随机波动，无明显模式。
# 2. 直方图+核密度估计: 残差分布应接近正态分布（钟形）。
# 3. Q-Q图: 点应基本落在红线上，表示残差是正态分布的。
# 4. 相关图(ACF): 残差的自相关系数不应有任何一个显著地超出置信区间（蓝色区域），这说明残差中没有剩余的自相关信息。Ljung-Box检验的p值(Prob(Q))应大于0.05。
model_fit.plot_diagnostics(figsize=(15, 12))
plt.suptitle('模型诊断图', fontsize=16, y=0.92)
plt.show()

# --- 步骤 5: 预测与可视化 ---

# 预测未来N个时间点
n_periods = 24  # 预测未来24个月
forecast, forecast_ci = model_fit.predict(n_periods=n_periods, return_conf_int=True)

# 创建未来的日期索引
future_index = pd.date_range(start=ts_data.index[-1] + pd.DateOffset(months=1), periods=n_periods, freq='MS')

# 将预测结果和置信区间转换为DataFrame
forecast_df = pd.DataFrame(forecast, index=future_index, columns=['预测值'])
conf_int_df = pd.DataFrame(forecast_ci, index=future_index, columns=['置信下限', '置信上限'])

print("--- 未来24期预测结果 ---")
print(forecast_df)
print("\n--- 置信区间 ---")
print(conf_int_df)
print("\n")

# 可视化预测结果
plt.figure(figsize=(15, 7))
# 绘制历史数据
plt.plot(ts_data, label='历史数据')
# 绘制预测数据
plt.plot(forecast_df, label='预测数据', color='red')
# 绘制置信区间
plt.fill_between(conf_int_df.index,
                 conf_int_df.iloc[:, 0],
                 conf_int_df.iloc[:, 1], color='pink', alpha=0.5, label='95%置信区间')

plt.title('ARIMA模型预测结果', fontsize=16)
plt.xlabel('日期')
plt.ylabel('数值')
plt.legend(loc='upper left')
plt.grid(True)
plt.show()
```

**输出结果：**

```
--- 数据预览 ---
2010-01-01    52.483571
2010-02-01    57.285931
2010-03-01    67.134656
2010-04-01    73.874347
2010-05-01    63.366282
Freq: MS, Name: value, dtype: float64


--- 对原始序列进行平稳性检验 ---
--- ADF检验结果 ---
ADF 统计量: 0.7525087015362423
p-value: 0.9908295754603504
临界值:
	1%: -3.4936021509366793
	5%: -2.8892174239808703
	10%: -2.58153320754717
结论: 序列是非平稳的 (无法拒绝原假设)


--- 绘制ACF和PACF图进行模型识别 ---
--- 对一阶差分序列进行平稳性检验 ---
--- ADF检验结果 ---
ADF 统计量: -7.690443279835323
p-value: 1.4239687239875478e-11
临界值:
	1%: -3.4936021509366793
	5%: -2.8892174239808703
	10%: -2.58153320754717
结论: 序列是平稳的 (拒绝原假设)


--- 对二阶差分序列进行平稳性检验 ---
--- ADF检验结果 ---
ADF 统计量: -9.512657488015064
p-value: 3.2183455076903586e-16
临界值:
	1%: -3.4948504603223145
	5%: -2.889758398668639
	10%: -2.5818220155325444
结论: 序列是平稳的 (拒绝原假设)


--- 使用auto_arima自动寻找最优模型 ---
Performing stepwise search to minimize bic
 ARIMA(1,1,1)(0,0,1)[12] intercept   : BIC=853.299, Time=0.12 sec
 ARIMA(0,1,0)(0,0,0)[12] intercept   : BIC=853.166, Time=0.01 sec
 ARIMA(1,1,0)(1,0,0)[12] intercept   : BIC=837.517, Time=0.06 sec
 ARIMA(0,1,1)(0,0,1)[12] intercept   : BIC=849.036, Time=0.07 sec
 ARIMA(0,1,0)(0,0,0)[12]             : BIC=848.701, Time=0.01 sec
 ARIMA(1,1,0)(0,0,0)[12] intercept   : BIC=857.678, Time=0.02 sec
 ARIMA(1,1,0)(2,0,0)[12] intercept   : BIC=830.314, Time=0.18 sec
 ARIMA(1,1,0)(2,0,1)[12] intercept   : BIC=818.295, Time=0.33 sec
 ARIMA(1,1,0)(1,0,1)[12] intercept   : BIC=813.706, Time=0.14 sec
 ARIMA(1,1,0)(0,0,1)[12] intercept   : BIC=848.818, Time=0.06 sec
 ARIMA(1,1,0)(1,0,2)[12] intercept   : BIC=818.221, Time=0.33 sec
 ARIMA(1,1,0)(0,0,2)[12] intercept   : BIC=848.207, Time=0.17 sec
 ARIMA(1,1,0)(2,0,2)[12] intercept   : BIC=inf, Time=0.63 sec
 ARIMA(0,1,0)(1,0,1)[12] intercept   : BIC=inf, Time=0.22 sec
 ARIMA(2,1,0)(1,0,1)[12] intercept   : BIC=814.285, Time=0.14 sec
 ARIMA(1,1,1)(1,0,1)[12] intercept   : BIC=inf, Time=0.54 sec
 ARIMA(0,1,1)(1,0,1)[12] intercept   : BIC=inf, Time=0.25 sec
 ARIMA(2,1,1)(1,0,1)[12] intercept   : BIC=inf, Time=0.37 sec
 ARIMA(1,1,0)(1,0,1)[12]             : BIC=809.029, Time=0.11 sec
 ARIMA(1,1,0)(0,0,1)[12]             : BIC=844.285, Time=0.04 sec
 ARIMA(1,1,0)(1,0,0)[12]             : BIC=832.892, Time=0.03 sec
 ARIMA(1,1,0)(2,0,1)[12]             : BIC=813.618, Time=0.27 sec
 ARIMA(1,1,0)(1,0,2)[12]             : BIC=813.544, Time=0.24 sec
 ARIMA(1,1,0)(0,0,0)[12]             : BIC=853.190, Time=0.01 sec
 ARIMA(1,1,0)(0,0,2)[12]             : BIC=843.650, Time=0.11 sec
 ARIMA(1,1,0)(2,0,0)[12]             : BIC=825.668, Time=0.11 sec
 ARIMA(1,1,0)(2,0,2)[12]             : BIC=inf, Time=0.51 sec
 ARIMA(0,1,0)(1,0,1)[12]             : BIC=inf, Time=0.22 sec
 ARIMA(2,1,0)(1,0,1)[12]             : BIC=809.609, Time=0.12 sec
 ARIMA(1,1,1)(1,0,1)[12]             : BIC=790.425, Time=0.15 sec
 ARIMA(1,1,1)(0,0,1)[12]             : BIC=848.767, Time=0.07 sec
 ARIMA(1,1,1)(1,0,0)[12]             : BIC=817.402, Time=0.08 sec
 ARIMA(1,1,1)(2,0,1)[12]             : BIC=795.052, Time=0.26 sec
 ARIMA(1,1,1)(1,0,2)[12]             : BIC=794.988, Time=0.29 sec
 ARIMA(1,1,1)(0,0,0)[12]             : BIC=857.412, Time=0.02 sec
 ARIMA(1,1,1)(0,0,2)[12]             : BIC=848.417, Time=0.22 sec
 ARIMA(1,1,1)(2,0,0)[12]             : BIC=804.015, Time=0.22 sec
 ARIMA(1,1,1)(2,0,2)[12]             : BIC=799.983, Time=0.40 sec
 ARIMA(0,1,1)(1,0,1)[12]             : BIC=788.774, Time=0.09 sec
 ARIMA(0,1,1)(0,0,1)[12]             : BIC=844.499, Time=0.04 sec
 ARIMA(0,1,1)(1,0,0)[12]             : BIC=816.103, Time=0.06 sec
 ARIMA(0,1,1)(2,0,1)[12]             : BIC=793.108, Time=0.20 sec
 ARIMA(0,1,1)(1,0,2)[12]             : BIC=792.958, Time=0.25 sec
 ARIMA(0,1,1)(0,0,0)[12]             : BIC=853.266, Time=0.01 sec
 ARIMA(0,1,1)(0,0,2)[12]             : BIC=843.923, Time=0.14 sec
 ARIMA(0,1,1)(2,0,0)[12]             : BIC=799.897, Time=0.15 sec
 ARIMA(0,1,1)(2,0,2)[12]             : BIC=inf, Time=0.68 sec
 ARIMA(0,1,2)(1,0,1)[12]             : BIC=790.859, Time=0.15 sec
 ARIMA(1,1,2)(1,0,1)[12]             : BIC=795.032, Time=0.20 sec

Best model:  ARIMA(0,1,1)(1,0,1)[12]          
Total fit time: 9.120 seconds

--- 自动选择的最优模型摘要 ---
                                     SARIMAX Results                                      
==========================================================================================
Dep. Variable:                                  y   No. Observations:                  120
Model:             SARIMAX(0, 1, 1)x(1, 0, 1, 12)   Log Likelihood                -384.829
Date:                            Fri, 18 Jul 2025   AIC                            777.657
Time:                                    15:45:24   BIC                            788.774
Sample:                                01-01-2010   HQIC                           782.171
                                     - 12-01-2019                                         
Covariance Type:                              opg                                         
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
ma.L1         -0.9053      0.052    -17.516      0.000      -1.007      -0.804
ar.S.L12       0.9892      0.010     98.168      0.000       0.969       1.009
ma.S.L12      -0.6980      0.110     -6.363      0.000      -0.913      -0.483
sigma2        29.9352      4.124      7.258      0.000      21.852      38.019
===================================================================================
Ljung-Box (L1) (Q):                   1.43   Jarque-Bera (JB):                 0.22
Prob(Q):                              0.23   Prob(JB):                         0.89
Heteroskedasticity (H):               0.89   Skew:                             0.11
Prob(H) (two-sided):                  0.72   Kurtosis:                         2.98
===================================================================================

Warnings:
[1] Covariance matrix calculated using the outer product of gradients (complex-step).


--- 模型诊断 ---
--- 未来24期预测结果 ---
                   预测值
2020-01-01  106.265838
...
2021-12-01  109.573949

--- 置信区间 ---
                  置信下限        置信上限
2020-01-01   95.539342  116.992334
...
2021-12-01   96.825016  122.322882
```

![](https://img.picgo.net/2025/07/18/25-07-18-1752827020229a303dd9d2bc3e523.webp)

![](https://img.picgo.net/2025/07/18/25-07-18-17528270348459ddc3711f042b03c.webp)

![](https://img.picgo.net/2025/07/18/25-07-18-1752827051978f0a21fb56cda5803.webp)

![](https://img.picgo.net/2025/07/18/25-07-18-17528270747634fd46e2982ade049.webp)

**结果解读：**

1. **数据特性与预处理**

**原始序列**：经ADF检验 (p-value=0.99)，原始序列为**非平稳**时间序列，含有趋势或周期性。
**差分处理**：进行**一阶差分 (d=1)** 后，序列变得平稳 (p-value ≈ 1.42e-11)，满足了ARIMA建模的前提

> 事实是在这个示例中差分可以被AR代替

2. **最优模型选择**

通过 `auto_arima` 自动寻优（基于BIC最小化原则），最终确定的最优模型为 **`SARIMAX(0, 1, 1)x(1, 0, 1, 12)`**

这是一个包含**一阶差分 (d=1)**、一个**移动平均项 (q=1)** 的模型，还包含了强大的**年度季节性结构 (m=12)**，由一个**季节性自回归项 (P=1)** 和一个**季节性移动平均项 (Q=1)** 共同驱动，`ar.S.L12` 系数 (0.9892) 极接近1，表明数据存在极强的年度周期性

3. **模型质量评估 (诊断检验)**
    * **残差独立性 (Ljung-Box)**: Prob(Q) (0.23) > 0.05，表明残差为白噪声，模型已充分提取信息
    * **残差正态性 (Jarque-Bera)**: Prob(JB) (0.89) > 0.05，表明残差服从正态分布
    * **残差方差齐性 (Heteroskedasticity)**: Prob(H) (0.72) > 0.05，表明残差方差恒定
    * 所有模型系数的p-value均为0.000，**统计上极其显著**


## 朴素贝叶斯分类方法

朴素贝叶斯（Naive Bayes）是一种基于贝叶斯定理（Bayes' Theorem）与特征条件独立性假设（Feature Conditional Independence Assumption）的概率分类算法。它以其简单、高效和在特定领域（尤其是文本分类）表现出奇的准确性而闻名


### 贝叶斯定理
贝叶斯定理是朴素贝叶斯算法的数学基石，它描述了在给定证据（特征）的情况下，一个假设（类别）的后验概率

其公式为：

$$
P(C|X) = \frac{P(X|C) \cdot P(C)}{P(X)}
$$

- **$P(C|X)$ (后验概率)**：我们最终的目标，表示在看到特征 $X$ 后，样本属于类别 $C$ 的概率
- **$P(X|C)$ (似然概率)**：在类别 $C$ 的条件下，观察到特征 $X$ 的概率。这是模型需要从训练数据中学习的部分
- **$P(C)$ (先验概率)**：在不考虑任何特征的情况下，类别 $C$ 发生的概率。可直接从训练数据中统计得出
- **$P(X)$ (证据因子)**：特征 $X$ 发生的概率。在分类决策中，对于所有类别 $C$，$P(X)$ 的值都是相同的，因此在比较不同类别的后验概率时，可以忽略此项

分类决策的核心是，计算样本 $X$ 属于每个类别 $C_k$ 的后验概率，并选择概率最大的类别作为预测结果

$$
\text{Predicted Class} = \arg\max_{C_k} P(C_k|X) \propto \arg\max_{C_k} P(X|C_k) \cdot P(C_k)
$$

### “朴素”的特征条件独立性假设

这是该算法“朴素”（Naive）一词的来源。它假设：**对于一个给定的类别，所有特征之间是相互独立的，互不影响**

在独立性假设下，似然概率 $P(X|C)$ 的计算被简化为：

$$
P(X|C) = P(x_1, x_2, ..., x_n|C) = \prod_{i=1}^{n} P(x_i|C)
$$

其中 $X=\{x_1, x_2, ..., x_n\}$ 是样本的 $n$ 个特征


### 核心步骤

1.  **准备阶段**：确定特征属性，并对训练样本进行分类
2.  **训练阶段**：
    -   计算每个类别的先验概率 $P(C_k)$
    -   计算每个类别下，每个特征属性的条件概率 $P(x_i|C_k)$
3.  **分类阶段**：
    -   对于一个新样本 $X$，带入公式计算它属于每个类别 $C_k$ 的后验概率（的分子部分）：$P(C_k) \prod_{i=1}^{n} P(x_i|C_k)$
    -   比较计算出的概率值，将样本归类到概率最大的那个类别

### 零概率问题与拉普拉斯平滑

**问题**：如果在训练集中，某个特征值在某个类别下从未出现过，会导致其条件概率 $P(x_i|C_k)$ 为 0。这会使得整个后验概率的乘积变为 0，从而“一票否决”该类别，这是不合理的

**拉普拉斯平滑 (Laplacian Smoothing)**，也称加一平滑。它对概率计算公式进行修正，确保任何概率值都不会为零
$$
P(x_i|C_k) = \frac{\text{类别 } C_k \text{ 中特征 } x_i \text{ 的出现次数} + \alpha}{\text{类别 } C_k \text{ 的总样本数} + \alpha \cdot K}
$$

- $\alpha$ 是平滑系数，通常取值为 1
- $K$ 是特征 $x_i$ 的可能取值总数

### 主要模型类型

根据特征数据的分布类型，朴素贝叶斯主要分为三种模型：

1.  **高斯朴素贝叶斯 (Gaussian Naive Bayes)**
    -   **适用场景**：特征是连续型数据（如身高、体重、温度），并假设这些特征在每个类别下都服从高斯（正态）分布
    -   **方法**：对每个类别下的每个特征，计算其均值和方差。在预测时，使用高斯概率密度函数来计算条件概率 $P(x_i|C_k)$

2.  **多项式朴素贝叶斯 (Multinomial Naive Bayes)**
    -   **适用场景**：特征是离散型数据，通常表示“出现次数”或“频率”
    -   **典型应用**：文本分类。特征是词汇表中的单词，值是该单词在文档中出现的次数（词频）

3.  **伯努利朴素贝叶斯 (Bernoulli Naive Bayes)**
    -   **适用场景**：特征是二元（布尔）数据，即只关心特征“是否出现”
    -   **典型应用**：文本分类。特征是词汇表中的单词，值是 1 或 0，表示该单词是否在文档中出现过

### 优缺点分析

#### 优点
* **算法简单高效**：模型训练和预测的速度非常快，易于实现
* **性能稳定**：对于小规模数据集，其表现通常很好，结果稳定
* **对缺失数据不敏感**：在计算概率时，可以简单地忽略缺失的特征
* **善于处理多分类问题**：在多分类任务中表现出色，且实现简单
* **在特征独立的场景下效果最佳**：当数据满足其核心假设时，效果可与更复杂的模型媲美

#### 缺点
* **特征条件独立性假设过强**：这个假设在现实世界中几乎不成立，是其主要理论缺陷。当特征之间关联性很强时，模型性能会下降
* **对输入数据的表达形式敏感**：不同的数据预处理（如分箱、转换）可能会对结果产生较大影响
* **先验概率影响**：在训练集不平衡时，先验概率可能会主导后验概率，导致对小样本类别的预测出现偏差

### 示例代码

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import load_iris
from sklearn.feature_extraction.text import CountVectorizer

# ==============================================================================
# 示例 1: GaussianNB (高斯朴素贝叶斯) - 用于连续特征
# ==============================================================================
print("="*60)
print("示例 1: GaussianNB (高斯朴素贝叶斯) on Iris Dataset")
print("="*60)

# 1. 加载数据集
# scikit-learn 自带了鸢尾花数据集
iris = load_iris()
X = iris.data  # 特征数据
y = iris.target # 标签（分类结果）

# 将数据转换成 DataFrame 格式，方便查看
df = pd.DataFrame(X, columns=iris.feature_names)
df['target'] = y
print("--- 数据集预览 (前5行) ---")
print(df.head())
print("\n标签含义: 0='setosa', 1='versicolor', 2='virginica'")

# 2. 划分训练集和测试集
# 将80%的数据用作训练，20%用作测试
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\n训练集大小: {X_train.shape}")
print(f"测试集大小: {X_test.shape}")

# 3. 初始化并训练朴素贝叶斯模型
# 因为特征是连续的数值，我们选择高斯朴素贝叶斯 (GaussianNB)
model = GaussianNB()

# 使用训练数据来训练模型
# .fit() 方法会计算每个类别的先验概率 P(C) 和每个特征在每个类别下的均值和方差
model.fit(X_train, y_train)
print("\n--- 模型训练完成 ---")

# 4. 使用模型进行预测
# 用训练好的模型对测试集进行预测
y_pred = model.predict(X_test)

print("\n--- 预测结果 vs 真实结果 ---")
print("预测结果:", y_pred)
print("真实结果:", y_test)


# 5. 评估模型性能
accuracy = accuracy_score(y_test, y_pred)
print(f"\n模型准确率 (Accuracy): {accuracy:.4f}")

# 查看更详细的评估报告，包括精确率、召回率、F1分数
print("\n--- 分类报告 (Classification Report) ---")
print(classification_report(y_test, y_pred, target_names=iris.target_names))

# 6. 预测一个新样本
# 假设我们有一朵新的鸢尾花，其特征为 [花萼长度, 花萼宽度, 花瓣长度, 花瓣宽度]
new_flower = [[5.1, 3.5, 1.4, 0.2]] # 这组数据很像 setosa
prediction = model.predict(new_flower)
predicted_class_name = iris.target_names[prediction[0]]

print(f"\n--- 预测新样本 ---")
print(f"新样本 {new_flower} 的预测类别是: {predicted_class_name} ({prediction[0]})")


# ==============================================================================
# 示例 2: MultinomialNB (多项式朴素贝叶斯) - 用于文本分类 (词频)
# ==============================================================================
print("="*60)
print("示例 2: MultinomialNB (多项式朴素贝叶斯) for Text Classification")
print("="*60)

# 1. 创建文本数据
# 假设我们有一些关于体育和非体育的文档
corpus = [
    'A great game of football',
    'The election was close',
    'A very clean sweep in the game',
    'The president is speaking',
    'The player scored a goal'
]
labels = ['sports', 'not sports', 'sports', 'not sports', 'sports']

# 2. 文本特征提取 (词频向量)
# CountVectorizer 会将文本转换为单词计数的向量
vectorizer_multi = CountVectorizer()
X_text_multi = vectorizer_multi.fit_transform(corpus)

# 将稀疏矩阵转换为DataFrame以便查看
df_multi = pd.DataFrame(X_text_multi.toarray(), columns=vectorizer_multi.get_feature_names_out())
print("文本数据的词频特征 (供 MultinomialNB 使用):")
print(df_multi)
print("-" * 30)

# 3. 训练模型 (这里为了演示，使用全部数据训练和预测)
mnb = MultinomialNB()
mnb.fit(X_text_multi, labels)

# 4. 预测与评估
y_pred_mnb = mnb.predict(X_text_multi)
accuracy_mnb = accuracy_score(labels, y_pred_mnb)

print(f"\nMultinomialNB 模型准确率: {accuracy_mnb:.4f}\n")
print("MultinomialNB 分类报告:")
print(classification_report(labels, y_pred_mnb))

# 预测新句子
new_sentence_multi = ['The team won the game']
new_vec_multi = vectorizer_multi.transform(new_sentence_multi)
prediction_mnb = mnb.predict(new_vec_multi)
print(f"预测新句子 '{new_sentence_multi[0]}' 的类别是: {prediction_mnb[0]}")
print("\n\n")


# ==============================================================================
# 示例 3: BernoulliNB (伯努利朴素贝叶斯) - 用于文本分类 (二元特征)
# ==============================================================================
print("="*60)
print("示例 3: BernoulliNB (伯努利朴素贝叶斯) for Text Classification")
print("="*60)

# 1. 创建文本数据 (与上面相同)
# 2. 文本特征提取 (二元特征)
# 使用 CountVectorizer 并设置 binary=True，它只关心单词是否出现，不关心次数
vectorizer_bern = CountVectorizer(binary=True)
X_text_bern = vectorizer_bern.fit_transform(corpus)

# 将稀疏矩阵转换为DataFrame以便查看
df_bern = pd.DataFrame(X_text_bern.toarray(), columns=vectorizer_bern.get_feature_names_out())
print("文本数据的二元特征 (供 BernoulliNB 使用):")
print(df_bern)
print("-" * 30)


# 3. 训练模型
bnb = BernoulliNB()
bnb.fit(X_text_bern, labels)

# 4. 预测与评估
y_pred_bnb = bnb.predict(X_text_bern)
accuracy_bnb = accuracy_score(labels, y_pred_bnb)

print(f"\nBernoulliNB 模型准确率: {accuracy_bnb:.4f}\n")
print("BernoulliNB 分类报告:")
print(classification_report(labels, y_pred_bnb))

# 预测新句子
new_sentence_bern = ['The president won the election']
new_vec_bern = vectorizer_bern.transform(new_sentence_bern)
prediction_bnb = bnb.predict(new_vec_bern)
print(f"预测新句子 '{new_sentence_bern[0]}' 的类别是: {prediction_bnb[0]}")
print("\n\n")
```

