---
banner_img: https://img.picgo.net/2025/03/28/25-03-28-1743150706296f3a97dda5694d36a.webp
index_img: https://img.picgo.net/2025/03/28/25-03-28-1743150706296586d07573c738663.webp
title: 机器学习笔记
date: 2025-03-28 16:32:50
updated: 2025-03-28 16:32:48
tags:
  - 机器学习
categories:
  - 学习笔记
comments: true
---
## 1. 监督学习

监督学习是已经知道数据的label，例如预测房价问题，给出了房子的面积和价格

* 回归问题是预测连续值的输出，例如预测房价

![image-20200922232432315](https://img.picgo.net/2025/03/28/25-03-28-1743143619246414111475a027645.png)

* 分类问题是预测离散值输出，例如判断肿瘤是良性还是恶性

![image-20200922232452094](https://img.picgo.net/2025/03/28/25-03-28-1743143695127ad770c735a0fc083.webp)

## 2. 无监督学习

无监督学习是不知道数据具体的含义，比如给定一些数据但不知道它们具体的信息，对于分类问题无监督学习可以得到多个不同的聚类，从而实现预测的功能

![image-20200922232542888](https://img.picgo.net/2025/03/28/25-03-28-17431438061779e4e3706e0702341.webp)

## 3. 线性回归

### 3.1 代价函数

![p-JSHk0GQDyiUh5NBmA8Mg_33a5e12f624140d1a9afe4376c1f75a1_w1l3ivq_2](https://s1.imagehub.cc/images/2025/03/28/69b495174b9398e22194dcf74f999504.webp)

![image145946](https://img.picgo.net/2025/03/28/25-03-28-1743145270302c696bd8423535a9d.webp)

### 3.2 梯度下降

梯度下降，首先为每个参数赋一个初值，通过代价函数的梯度，然后不断地调整参数，最终得到一个局部最优解。初值的不同可能会得到两个不同的结果，即梯度下降不一定得到全局最优解

![屏幕截图 2025-03-28 145114](https://img.picgo.net/2025/03/28/25-03-28-174314479530291a8cd37389b8d2c.webp)

梯度下降在具体的执行时，每一次更新需要同时更新所有的参数。

![屏幕截图 2025-03-28 150321](https://img.picgo.net/2025/03/28/25-03-28-1743145542315d477ee9cf96be051.webp)

![](https://img.picgo.net/2025/03/28/25-03-28-1743145895067fedf5b6eb8369fdf.webp)

梯度下降效果图示

![](https://s1.imagehub.cc/images/2025/03/16/b3660e0a2d9d4dfc4d4e199908b92671.png)

梯度下降过程容易出现局部最优解

![屏幕截图 2025-03-28 151525](https://img.picgo.net/2025/03/28/25-03-28-17431461519203882e40b5917ae6d.webp)

但是线性回归的代价函数往往是凸函数，总能收敛到全局最优解

![屏幕截图 2025-03-28 151609](https://img.picgo.net/2025/03/28/25-03-28-17431462243244c0997b3075442e3.webp)

**学习率:** 过小下降慢，过大可能无法收敛

![屏幕截图 2025-03-28 151945](https://img.picgo.net/2025/03/28/25-03-28-17431464178914c3c8e41debe543c.webp)
