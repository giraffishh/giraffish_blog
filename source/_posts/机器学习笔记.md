---
abbrlink: "0"
banner_img: https://img.picgo.net/2025/03/28/25-03-28-17431514678427b2138c37794ab42.webp
index_img: https://img.picgo.net/2025/03/28/25-03-28-1743150706296586d07573c738663.webp
title: 机器学习笔记
date: 2025-03-28 16:32:50
updated: 2025-03-29 18:03:47
tags:
  - 机器学习
categories:
  - 学习笔记
comments: true
---
[**课程主页**](https://www.coursera.org/specializations/machine-learning-introduction/?utm_medium=coursera&utm_source=home-page&utm_campaign=mlslaunch2022IN)
[**配套练习代码**](https://github.com/greyhatguy007/Machine-Learning-Specialization-Coursera)


## 1. 监督学习

监督学习是已经知道数据的label，例如预测房价问题，给出了房子的面积和价格

* 回归问题是预测连续值的输出，例如预测房价

![](https://img.picgo.net/2025/03/28/25-03-28-1743143619246414111475a027645.png)

* 分类问题是预测离散值输出，例如判断肿瘤是良性还是恶性

![](https://img.picgo.net/2025/03/28/25-03-28-1743143695127ad770c735a0fc083.webp)

## 2. 无监督学习

无监督学习是不知道数据具体的含义，比如给定一些数据但不知道它们具体的信息，对于分类问题无监督学习可以得到多个不同的聚类，从而实现预测的功能

![](https://img.picgo.net/2025/03/28/25-03-28-17431438061779e4e3706e0702341.webp)

## 3. 线性回归

### 3.1 代价函数

![](https://s1.imagehub.cc/images/2025/03/28/69b495174b9398e22194dcf74f999504.webp)

![](https://img.picgo.net/2025/03/28/25-03-28-1743145270302c696bd8423535a9d.webp)

### 3.2 梯度下降

梯度下降，首先为每个参数赋一个初值，通过代价函数的梯度，然后不断地调整参数，最终得到一个局部最优解。初值的不同可能会得到两个不同的结果，即梯度下降不一定得到全局最优解

![](https://img.picgo.net/2025/03/28/25-03-28-174314479530291a8cd37389b8d2c.webp)

梯度下降在具体的执行时，每一次更新需要同时更新所有的参数。

![](https://img.picgo.net/2025/03/28/25-03-28-1743145542315d477ee9cf96be051.webp)

![](https://img.picgo.net/2025/03/28/25-03-28-1743145895067fedf5b6eb8369fdf.webp)

梯度下降效果图示

![](https://s1.imagehub.cc/images/2025/03/16/b3660e0a2d9d4dfc4d4e199908b92671.png)

梯度下降过程容易出现局部最优解

![](https://img.picgo.net/2025/03/28/25-03-28-17431461519203882e40b5917ae6d.webp)

但是线性回归的代价函数往往是凸函数，总能收敛到全局最优解

![](https://img.picgo.net/2025/03/28/25-03-28-17431462243244c0997b3075442e3.webp)

**学习曲线：**

![](https://img.picgo.net/2025/03/29/25-03-29-1743233763258d724c0d3c06f17a7.webp)

**学习率:** 过小下降慢，过大可能无法收敛

![](https://img.picgo.net/2025/03/28/25-03-28-17431464178914c3c8e41debe543c.webp)

寻找最佳学习率

![](https://img.picgo.net/2025/03/29/25-03-29-1743234261143a42e768db385b2ff.webp)

**多元梯度下降与向量化**

通常问题都会涉及到多个变量，例如房屋价格预测就包括，面积、房间个数、楼层、价格等

![](https://img.picgo.net/2025/03/29/25-03-29-1743232186021a8e30f67e5c2cec3.webp)

而向量化进行矩阵运算可以显著提高运算效率

![](https://img.picgo.net/2025/03/29/25-03-29-1743232285309ef96d4d895379443.webp)

$$
\mathbf{X} = 
\begin{pmatrix}
 x^{(0)}_0 & x^{(0)}_1 & \cdots & x^{(0)}_{n-1} \\ 
 x^{(1)}_0 & x^{(1)}_1 & \cdots & x^{(1)}_{n-1} \\
 \cdots \\
 x^{(m-1)}_0 & x^{(m-1)}_1 & \cdots & x^{(m-1)}_{n-1} 
\end{pmatrix}
$$

**Notation:**

* $\mathbf{x}^{(i)}$ is vector containing example i. $\mathbf{x}^{(i)}$ $ = (x^{(i)}_0, x^{(i)}_1, \cdots,x^{(i)}_{n-1})$
* $x^{(i)}_j$ is element j in example i. The superscript in parenthesis indicates the example number while the subscript represents an element. 

$$
\mathbf{w} = \begin{pmatrix}
w_0 \\ 
w_1 \\
\cdots\\
w_{n-1}
\end{pmatrix}
$$

$$
f_{\mathbf{w},b}(\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b = \mathbf{w} \cdot \mathbf{x} + b
$$

### 3.3 正规方程

正规方程（Normal Equation）是一种 **解析解** 方法，可直接求解线性回归的最优参数，无需迭代优化。

假设线性回归模型为：
$$
y = XW + \varepsilon
$$
其中：
- $X$ 是特征矩阵（包含偏置项 $x_0 = 1$）。
- $W$ 是待求参数向量。
- $y$ 是目标值向量。
- $\varepsilon$ 是误差项。

最优参数 $W$ 由正规方程计算：
$$
W = (X^T X)^{-1} X^T y
$$
其中：
- $X^T X$ 是一个 **正规矩阵**（为了求逆转为方阵，正规矩阵可以用酉矩阵对角化）
- $(X^T X)^{-1}$ 是它的逆矩阵（如果可逆）
- $X^T y$​ 是特征与目标值的内积

正规方程求解方法与梯度下降对比：

![](https://img.picgo.net/2025/03/29/25-03-29-17432363753406b71859f8f29bd69.webp)

> 正规方程只能求解线性回归，且计算 $(X^T X)^{-1}$ 需要 $O(n^3)$ 复杂度，计算成本高

### 3.4 特征缩放

多个变量的度量不同，数字之间相差的大小也不同，如果可以将所有的特征变量缩放到大致相同范围，这样会减少梯度算法的迭代

特征缩放不一定非要落到[-1，1]之间，只要数据足够接近就可以

![](https://img.picgo.net/2025/03/29/25-03-29-17432331170437198a4025fc9d77a.webp)

**法一：均值归一化**

![](https://img.picgo.net/2025/03/29/25-03-29-17432331710286dab88e5ee871889.webp)

**法二：Z-score 标准化**

![](https://img.picgo.net/2025/03/29/25-03-29-1743233272396157041f20d1c1c6b.webp)

### 3.5 特征工程

通过转换和组合原始特征，形成新的特征，能使学习算法做出更准确的预测

![](https://img.picgo.net/2025/03/29/25-03-29-1743234876149af5a3aa0ed89f70d.webp)

### 3.6 多项式回归

![](https://img.picgo.net/2025/03/29/25-03-29-17432394184991cc882f35e5f1cc6.webp)
